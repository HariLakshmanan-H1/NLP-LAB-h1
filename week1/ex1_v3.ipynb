{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03c91b6c",
   "metadata": {},
   "source": [
    "DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8283cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f57065b",
   "metadata": {},
   "source": [
    "STATISTICAL MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbda9c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83b9ad8",
   "metadata": {},
   "source": [
    "STEMMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724ad088",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf07396",
   "metadata": {},
   "source": [
    "TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6dee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = \"The generic general currently planned the organ organization. The women left the leaves by the bank.\"                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf03fdb",
   "metadata": {},
   "source": [
    "SENTENCE TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbbb8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "\n",
    "nested_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "print(f\"Sentences : {sentences}\")\n",
    "print(f\"Tokens : {nested_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f29fa2f",
   "metadata": {},
   "source": [
    "TAGGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c04efc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN # Default fallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce6750c",
   "metadata": {},
   "source": [
    "for i, sentence in enumerate(sentences,1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TEST SENTENCE {i}: \\\"{sentence}\\\"\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(sentence)\n",
    "    \n",
    "    # Get POS Tags for the Lemmatizer\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    # Print Header\n",
    "    print(f\"{'Token':<15} | {'Porter':<15} | {'Lancaster':<15} | {'Snowball':<15} | {'Lemmatizer':<15}\")\n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    for token, tag in pos_tags:\n",
    "        # Apply Stemmers\n",
    "        p_stem = porter.stem(token)\n",
    "        l_stem = lancaster.stem(token)\n",
    "        s_stem = snowball.stem(token)\n",
    "        \n",
    "        # Apply Lemmatizer (using the dynamic POS tag)\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "        lemma = lemmatizer.lemmatize(token, wn_tag)\n",
    "        \n",
    "        print(f\"{token:<15} | {p_stem:<15} | {l_stem:<15} | {s_stem:<15} | {lemma:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe2018e",
   "metadata": {},
   "source": [
    "STEMMED SENTENCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340fe2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_sentences = [[porter.stem(token) for token in tokens] for tokens in nested_tokens]\n",
    "l_sentences = [[lancaster.stem(token) for token in tokens] for tokens in nested_tokens]\n",
    "s_sentences = [[snowball.stem(token) for token in tokens] for tokens in nested_tokens]\n",
    "\n",
    "print(p_sentences)\n",
    "print(l_sentences)\n",
    "print(s_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414ef713",
   "metadata": {},
   "source": [
    "TAGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c9260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags_nested = [nltk.pos_tag(tokens) for tokens in nested_tokens]\n",
    "print(pos_tags_nested)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfb8506",
   "metadata": {},
   "source": [
    "LEMMATIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e54d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_sentences = []\n",
    "for tokens, pos_tags in zip(nested_tokens, pos_tags_nested):\n",
    "    lemmas = []\n",
    "    for token, tag in pos_tags:\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "        lemma = lemmatizer.lemmatize(token, wn_tag)\n",
    "        lemmas.append(lemma)\n",
    "    lemmatized_sentences.append(lemmas)\n",
    "\n",
    "print(lemmatized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3c8c38",
   "metadata": {},
   "source": [
    "PORTER GRAMMAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf69bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import CFG,ChartParser\n",
    "\n",
    "porter_grammar = CFG.fromstring(\"\"\"\n",
    "    S -> NP VP Punct\n",
    "    NP -> Det N | Det ADJ N | Det N N\n",
    "    VP -> Adv V NP | V PP | V NP PP\n",
    "    PP -> P NP\n",
    "    Det -> 'The' | 'the'  \n",
    "    N -> 'gener' | 'organ' | 'women' | 'leav' | 'bank'\n",
    "    ADJ -> 'gener'\n",
    "    V -> 'plan' | 'left'\n",
    "    Adv -> 'current'\n",
    "    P -> 'by'\n",
    "    Punct -> '.'\n",
    "\n",
    "    \"\"\")\n",
    "\n",
    "porter_parser = ChartParser(porter_grammar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d896df",
   "metadata": {},
   "source": [
    "PORTER PARSE TREE CONSTRUCTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33241ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_tree1 = list(porter_parser.parse(p_sentences[0]))\n",
    "porter_tree2 = list(porter_parser.parse(p_sentences[1]))\n",
    "\n",
    "print(f\"Found {len(porter_tree1)} valid parse trees for sentence 1\\n\")\n",
    "for i, tree in enumerate(porter_tree1, 1):\n",
    "    print(f\"Tree #{i} Interpretation:\")\n",
    "    tree.pretty_print()\n",
    "\n",
    "\n",
    "print(f\"Found {len(porter_tree2)} valid parse trees for sentence 2\\n\")\n",
    "for i, tree in enumerate(porter_tree2, 1):\n",
    "    print(f\"Tree #{i} Interpretation:\")\n",
    "    tree.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6674cc",
   "metadata": {},
   "source": [
    "LANCASTER GRAMMAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30425a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "lancaster_grammar = CFG.fromstring(\"\"\"\n",
    "    # --- STRUCTURE ---\n",
    "    S -> NP VP PUNCT\n",
    "    \n",
    "    # NP Rules: \n",
    "    # Det N N covers \"gen gen\" (noun-noun) AND \"org org\"\n",
    "    # Det ADJ N covers \"gen gen\" (adj-noun) \n",
    "    NP -> Det N | Det ADJ N | Det N N\n",
    "    \n",
    "    # VP Rules: Matches both sentence structures\n",
    "    VP -> Adv V NP | V NP PP\n",
    "    PP -> P NP\n",
    "    \n",
    "    # --- LANCASTER VOCABULARY ---\n",
    "    Det -> 'the'\n",
    "    N   -> 'gen' | 'org' | 'wom' | 'leav' | 'bank'\n",
    "    ADJ -> 'gen'\n",
    "    V   -> 'plan' | 'left'\n",
    "    Adv -> 'cur'\n",
    "    P   -> 'by'\n",
    "    PUNCT -> '.'\n",
    "\"\"\")\n",
    "\n",
    "lancaster_parser = ChartParser(lancaster_grammar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baecb9fd",
   "metadata": {},
   "source": [
    "LANCASTER PARSE TREE CONSTRUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f7b506",
   "metadata": {},
   "outputs": [],
   "source": [
    "lancaster_tree1 = list(lancaster_parser.parse(l_sentences[0]))\n",
    "lancaster_tree2 = list(lancaster_parser.parse(l_sentences[1]))\n",
    "\n",
    "print(f\"Found {len(lancaster_tree1)} valid parse trees for sentence 1\\n\")\n",
    "for i, tree in enumerate(lancaster_tree1, 1):\n",
    "    print(f\"Tree #{i} Interpretation:\")\n",
    "    tree.pretty_print()\n",
    "\n",
    "\n",
    "print(f\"Found {len(lancaster_tree2)} valid parse trees for sentence 2\\n\")\n",
    "for i, tree in enumerate(lancaster_tree2, 1):\n",
    "    print(f\"Tree #{i} Interpretation:\")\n",
    "    tree.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce1af77",
   "metadata": {},
   "source": [
    "SNOWBALL GRAMMAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6634512d",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball_grammar = CFG.fromstring(\"\"\"\n",
    "    # --- STRUCTURE ---\n",
    "    S -> NP VP PUNCT\n",
    "    \n",
    "    NP -> Det N | Det ADJ N | Det N N\n",
    "    VP -> Adv V NP | V NP PP\n",
    "    PP -> P NP\n",
    "    \n",
    "    # --- SNOWBALL VOCABULARY ---\n",
    "    Det -> 'the'\n",
    "    \n",
    "    # Notice: 'generic' is NOT here, it is in ADJ\n",
    "    N   -> 'general' | 'organ' | 'women' | 'leav' | 'bank'\n",
    "    \n",
    "    # Distinct Adjective!\n",
    "    ADJ -> 'generic'\n",
    "    \n",
    "    V   -> 'plan' | 'left'\n",
    "    Adv -> 'current'\n",
    "    P   -> 'by'\n",
    "    PUNCT -> '.'\n",
    "\"\"\")\n",
    "\n",
    "snowball_parser = ChartParser(snowball_grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a324a4",
   "metadata": {},
   "source": [
    "SNOWBALL PARSE TREE CONSTRUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391c9089",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball_tree1 = list(snowball_parser.parse(s_sentences[0]))\n",
    "snowball_tree2 = list(snowball_parser.parse(s_sentences[1]))\n",
    "\n",
    "print(f\"Found {len(snowball_tree1)} valid parse trees for sentence 1\\n\")\n",
    "for i, tree in enumerate(snowball_tree1, 1):\n",
    "    print(f\"Tree #{i} Interpretation:\")\n",
    "    tree.pretty_print()\n",
    "\n",
    "\n",
    "print(f\"Found {len(snowball_tree2)} valid parse trees for sentence 2\\n\")\n",
    "for i, tree in enumerate(snowball_tree2, 1):\n",
    "    print(f\"Tree #{i} Interpretation:\")\n",
    "    tree.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099753ac",
   "metadata": {},
   "source": [
    "LEMMA GRAMMAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288fa864",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_grammar = CFG.fromstring(\"\"\"\n",
    "    # --- STRUCTURE ---\n",
    "    S -> NP VP PUNCT\n",
    "    \n",
    "    NP -> Det N | Det ADJ N | Det N N\n",
    "    VP -> Adv V NP | V NP PP\n",
    "    PP -> P NP\n",
    "    \n",
    "    # --- LEMMATIZER VOCABULARY ---\n",
    "    # Case Sensitive!\n",
    "    Det -> 'The' | 'the'\n",
    "    \n",
    "    # Full words restored\n",
    "    N   -> 'general' | 'organ' | 'organization' | 'woman' | 'leaf' | 'bank'\n",
    "    ADJ -> 'generic'\n",
    "    \n",
    "    # Verbs in root form\n",
    "    V   -> 'plan' | 'leave'\n",
    "    Adv -> 'currently'\n",
    "    P   -> 'by'\n",
    "    PUNCT -> '.'\n",
    "\"\"\")\n",
    "\n",
    "lemma_parser = ChartParser(lemma_grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07034323",
   "metadata": {},
   "source": [
    "LEMMA PARSE TREE CONSTRUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4a7d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_tree1 = list(lemma_parser.parse(lemmatized_sentences[0]))\n",
    "lemma_tree2 = list(lemma_parser.parse(lemmatized_sentences[1]))\n",
    "\n",
    "print(f\"Found {len(lemma_tree1)} valid parse trees for sentence 1\\n\")\n",
    "for i, tree in enumerate(lemma_tree1, 1):\n",
    "    print(f\"Tree #{i} Interpretation:\")\n",
    "    tree.pretty_print()\n",
    "\n",
    "\n",
    "print(f\"Found {len(lemma_tree2)} valid parse trees for sentence 2\\n\")\n",
    "for i, tree in enumerate(lemma_tree2, 1):\n",
    "    print(f\"Tree #{i} Interpretation:\")\n",
    "    tree.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f2880d",
   "metadata": {},
   "source": [
    "NAMED ENTITY RECOGNITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7935f664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "nltk.download('maxent_ne_chunker_tab', quiet=True)\n",
    "nltk.download('words', quiet=True)\n",
    "\n",
    "\n",
    "tags = nltk.pos_tag([t.capitalize() for t in nested_tokens[1]])\n",
    "# print(nested_tokens[0])\n",
    "\n",
    "\n",
    "ner_tags = nltk.ne_chunk(tags)\n",
    "# print(ner_tags)\n",
    "print(\"Named Entity Recognition Output:\\n\")\n",
    "\n",
    "for entity in ner_tags:\n",
    "    if isinstance(entity, nltk.Tree):\n",
    "        \n",
    "        entity_words = [word for word, tag in entity.leaves()]\n",
    "        entity_name = \" \".join(entity_words)\n",
    "        entity_label = entity.label()\n",
    "        \n",
    "        print(f\"Entity: {entity_name}, Label: {entity_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75715b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921907e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(text.capitalize())\n",
    "print(text)\n",
    "for entity in doc.ents:\n",
    "    print(f\"Entity: {entity.text}, Label: {entity.label_}\")\n",
    "\n",
    "\n",
    "t = \"Macavity is a Mystery Person in Madurai: heâ€™s called the CEO of Hidden Paw!\"\n",
    "doc1 = nlp(t)\n",
    "for entity in doc1.ents:\n",
    "    print(f\"Entity: {entity.text}, Label: {entity.label_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab7b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet',quiet=True)\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('omw-1.4',quiet=True)\n",
    "\n",
    "def get_semantic_info(word):\n",
    "    synsets = wn.synsets(word)\n",
    "    if not synsets:\n",
    "        return None\n",
    "    synset = synsets[0]\n",
    "    return {\n",
    "        \"Definition\": synset.definition(),\n",
    "        \"Hypernyms\" : [h.name() for h in synset.hypernyms()],\n",
    "        \"Hyponyms\" : [h.name() for h in synset.hyponyms()],\n",
    "        \"Root\": synset.root_hypernyms()[0].name()\n",
    "    }\n",
    "\n",
    "bliss_semantics = get_semantic_info('cat')\n",
    "\n",
    "print(bliss_semantics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d757345",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords',quiet=True)\n",
    "\n",
    "english_stops = set(stopwords.words('english'))\n",
    "\n",
    "t = \"Macavity is not guilty in Madurai ; he is just a misunderstood feline\"\n",
    "words = t.split()\n",
    "\n",
    "[word for word in words if word not in\n",
    "english_stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5cd846",
   "metadata": {},
   "outputs": [],
   "source": [
    "class word_syn_replacer(object):\n",
    "\n",
    "    def __init__(self, word_map):\n",
    "        self.word_map = word_map\n",
    "\n",
    "    def replace(self, word):\n",
    "        return self.word_map.get(word, word)\n",
    "\n",
    "rep_syn = word_syn_replacer({'bday' :\n",
    "'birthday'})\n",
    "\n",
    "rep_syn.replace('bday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea38a4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "class word_antonym_replacer(object):\n",
    "    def replace(self, word, pos=None):\n",
    "        antonyms = set()\n",
    "        for syn in wordnet.synsets(word, pos=pos):\n",
    "            for lemma in syn.lemmas():\n",
    "                for antonym in lemma.antonyms():\n",
    "                    antonyms.add(antonym.name())\n",
    "        \n",
    "        # CORRECT LOGIC: Check length AFTER collecting all candidates\n",
    "        if len(antonyms) == 1:\n",
    "            return antonyms.pop()\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def replace_negations(self, sent):\n",
    "        i, l = 0, len(sent)\n",
    "        words = []\n",
    "        \n",
    "        while i < l:\n",
    "            word = sent[i]\n",
    "            \n",
    "            # Check for \"not\" + valid next word\n",
    "            if word == 'not' and i+1 < l:\n",
    "                ant = self.replace(sent[i+1])\n",
    "                \n",
    "                if ant:\n",
    "                    words.append(ant)\n",
    "                    i += 2\n",
    "                    continue\n",
    "            \n",
    "            # This part runs if:\n",
    "            # 1. Word is NOT 'not'\n",
    "            # 2. Word IS 'not' but no antonym was found\n",
    "            words.append(word)\n",
    "            i += 1\n",
    "            \n",
    "        return words\n",
    "\n",
    "# Usage\n",
    "rep_antonym = word_antonym_replacer()\n",
    "result = rep_antonym.replace_negations(words)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ebb7b1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
